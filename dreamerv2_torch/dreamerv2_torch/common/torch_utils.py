import pathlib
import pickle
import re

import numpy as np
import torch
import torch.nn as nn


class Module(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self._lazy_modules = nn.ModuleDict({})

    def get(self, name, ctor, *args, **kwargs):
        # Create or get layer by name to avoid mentioning it in the constructor.
        if name not in self._lazy_modules:
            self._lazy_modules[name] = ctor(*args, **kwargs)
        return self._lazy_modules[name]


class Optimizer(nn.Module):
    def __init__(
        self,
        name,
        parameters,
        lr,
        eps=1e-4,
        clip=None,
        wd=None,
        wd_pattern=r".*",
        opt="adam",
        use_amp=False,
    ):
        super().__init__()
        assert 0 <= wd < 1
        assert not clip or 1 <= clip
        self._name = name
        self._params = list(parameters)
        self._clip = clip
        self._wd = wd
        self._wd_pattern = wd_pattern
        self._opt = {
            "adam": lambda: torch.optim.Adam(self._params, lr=lr, eps=eps),
            "nadam": lambda: NotImplemented(f"{config.opt} is not implemented"),
            "adamax": lambda: torch.optim.Adamax(self._params, lr=lr, eps=eps),
            "sgd": lambda: torch.optim.SGD(self._params, lr=lr),
            "momentum": lambda: torch.optim.SGD(self._params, lr=lr, momentum=0.9),
        }[opt]()
        self._scaler = torch.cuda.amp.GradScaler(enabled=use_amp)

    def __call__(self, loss, retain_graph=False):
        assert len(loss.shape) == 0, loss.shape
        metrics = {}
        metrics[f"{self._name}_loss"] = loss.detach().cpu().numpy()
        self._scaler.scale(loss).backward()
        self._scaler.unscale_(self._opt)
        # loss.backward(retain_graph=retain_graph)
        norm = torch.nn.utils.clip_grad_norm_(self._params, self._clip)
        if self._wd:
            self._apply_weight_decay(self._params)
        self._scaler.step(self._opt)
        self._scaler.update()
        # self._opt.step()
        self._opt.zero_grad()
        metrics[f"{self._name}_grad_norm"] = norm.item()
        return metrics

    def _apply_weight_decay(self, varibs):
        nontrivial = self._wd_pattern != r".*"
        if nontrivial:
            raise NotImplementedError
        for var in varibs:
            var.data = (1 - self._wd) * var.data


class RequiresGrad:
    def __init__(self, model):
        self._model = model

    def __enter__(self):
        self._model.requires_grad_(requires_grad=True)

    def __exit__(self, *args):
        self._model.requires_grad_(requires_grad=False)
